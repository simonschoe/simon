{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VQGAN+CLIP.ipynb","private_outputs":true,"provenance":[{"file_id":"1_4Jl0a7WIJeqy5LTjPJfZOwMZopG5C-W","timestamp":1652022979393},{"file_id":"1go6YwMFe5MX6XM9tv-cnQiSTU50N9EeT","timestamp":1625858616986},{"file_id":"1L8oL-vLJXVcRzCFbPwOoMkPKJ8-aYdPN","timestamp":1621115030940},{"file_id":"15UwYDsnNeldJFHJ9NdgYBYeo6xPmSelP","timestamp":1618333741979}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"CppIQlPhhwhs"},"source":["# Generative Art Using VQGAN and CLIP"]},{"cell_type":"markdown","source":["### Setup"],"metadata":{"id":"dpBLQISRBXLY"}},{"cell_type":"code","source":["from google.colab import drive\n","from pathlib import Path\n","\n","drive.mount('/content/drive')\n","full_path = Path('/content/drive/MyDrive', 'image-synthesis')"],"metadata":{"id":"LfI_L4zYRqFs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"1CJkByJXTERO"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wSfISAhyPmyp"},"source":["# download vqgan and clip repos\n","!git clone https://github.com/CompVis/taming-transformers &> /dev/null\n","!git clone https://github.com/openai/CLIP &> /dev/null\n","\n","# ai relevant libraries\n","!pip install ftfy regex tqdm omegaconf pytorch-lightning kornia einops &> /dev/null\n","\n","# transformers and vqgan\n","!pip install transformers &> /dev/null  \n","!pip install taming.models &> /dev/null\n"," \n","# libraries for managing meta data\n","!pip install stegano &> /dev/null\n","!apt install exempi &> /dev/null\n","!pip install python-xmp-toolkit imgtag pillow==7.1.2 &> /dev/null\n","\n","# reload all modules\n","%reload_ext autoreload\n","%autoreload &> /dev/null\n","\n","# libraries for brand image creation\n","!pip install fire icon_font_to_png &> /dev/null\n","!git clone https://github.com/minimaxir/icon-image &> /dev/null\n"," \n","# libraries for video creation\n","!pip install imageio-ffmpeg &> /dev/null\n","\n","# libraries for super resolution upscaling\n","!git clone https://github.com/Mirwaisse/SRCNN.git\n","\n","!mkdir steps"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Selection"],"metadata":{"id":"lbV2R2Q6BhEI"}},{"cell_type":"code","metadata":{"id":"FhhdWrSxQhwg"},"source":["# VQGAN Model\n","\n","imagenet_1024 = False\n","imagenet_16384 = True\n","coco = False\n","wikiart_1024 = False\n","wikiart_16384 = False\n","faceshq = False\n","sflckr = False\n","\n","if imagenet_1024:\n","  model_alias = 'vqgan_imagenet_f16_1024'\n","  !curl -L -o vqgan_imagenet_f16_1024.ckpt -C - 'https://heibox.uni-heidelberg.de/f/140747ba53464f49b476/?dl=1' \n","  !curl -L -o vqgan_imagenet_f16_1024.yaml -C - 'https://heibox.uni-heidelberg.de/f/6ecf2af6c658432c8298/?dl=1' \n","if imagenet_16384:\n","  model_alias = 'vqgan_imagenet_f16_16384'\n","  !curl -L -o vqgan_imagenet_f16_16384.ckpt -C - 'https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1'\n","  !curl -L -o vqgan_imagenet_f16_16384.yaml -C - 'https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1'\n","if coco:\n","  model_alias = 'coco'\n","  !curl -L -o coco.yaml -C - 'https://dl.nmkd.de/ai/clip/coco/coco.yaml'\n","  !curl -L -o coco.ckpt -C - 'https://dl.nmkd.de/ai/clip/coco/coco.ckpt'\n","if faceshq:\n","  model_alias = 'faceshq'\n","  !curl -L -o faceshq.yaml -C - 'https://drive.google.com/uc?export=download&id=1fHwGx_hnBtC8nsq7hesJvs-Klv-P0gzT'\n","  !curl -L -o faceshq.ckpt -C - 'https://app.koofr.net/content/links/a04deec9-0c59-4673-8b37-3d696fe63a5d/files/get/last.ckpt?path=%2F2020-11-13T21-41-45_faceshq_transformer%2Fcheckpoints%2Flast.ckpt'\n","if wikiart_1024:\n","  model_alias = 'wikiart_1024'\n","  !curl -L -o wikiart_1024.yaml -C - 'http://mirror.io.community/blob/vqgan/wikiart.yaml'\n","  !curl -L -o wikiart_1024.ckpt -C - 'http://mirror.io.community/blob/vqgan/wikiart.ckpt'\n","if wikiart_16384:\n","  model_alias = 'wikiart_16384'\n","  !curl -L -o wikiart_16384.ckpt -C - 'http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.ckpt'\n","  !curl -L -o wikiart_16384.yaml -C - 'http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.yaml'\n","if sflckr:\n","  model_alias = 'sflckr'\n","  !curl -L -o sflckr.yaml -C - 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fconfigs%2F2020-11-09T13-31-51-project.yaml&dl=1'\n","  !curl -L -o sflckr.ckpt -C - 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fcheckpoints%2Flast.ckpt&dl=1'\n","\n","\n","# Super Resolution Model\n","\n","SR_FACTOR = 3\n","\n","if SR_FACTOR == 2:\n","    !curl https://raw.githubusercontent.com/justinjohn0306/SRCNN/master/models/model_2x.pth -o model_2x.pth\n","elif SR_FACTOR == 3:\n","    !curl https://raw.githubusercontent.com/justinjohn0306/SRCNN/master/models/model_3x.pth -o model_3x.pth\n","elif SR_FACTOR == 4:\n","    !curl https://raw.githubusercontent.com/justinjohn0306/SRCNN/master/models/model_4x.pth -o model_4x.pth"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load Libraries and Define Helpers"],"metadata":{"id":"BYHztaaoBszo"}},{"cell_type":"code","metadata":{"id":"EXMSuW2EQWsd"},"source":["import argparse\n","import shutil\n","import math\n","from pathlib import Path\n","import sys\n","\n","# sys.path is where Python searches for modules\n","sys.path.append('./taming-transformers')\n","\n","from IPython import display\n","from base64 import b64encode\n","from omegaconf import OmegaConf\n","from PIL import Image\n","from taming.models import cond_transformer, vqgan\n","import torch\n","from torch import nn, optim\n","from torch.nn import functional as F\n","from torchvision import transforms\n","from torchvision.transforms import functional as TF\n","from tqdm.notebook import tqdm\n"," \n","from CLIP import clip\n","import kornia.augmentation as K\n","import numpy as np\n","import imageio\n","from PIL import ImageFile, Image\n","from imgtag import ImgTag\n","from libxmp import *     \n","import libxmp            \n","from stegano import lsb\n","import json\n","\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n"," \n","def sinc(x):\n","    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n"," \n","def lanczos(x, a):\n","    cond = torch.logical_and(-a < x, x < a)\n","    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n","    return out / out.sum()\n"," \n","def ramp(ratio, width):\n","    n = math.ceil(width / ratio + 1)\n","    out = torch.empty([n])\n","    cur = 0\n","    for i in range(out.shape[0]):\n","        out[i] = cur\n","        cur += ratio\n","    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n"," \n","def resample(input, size, align_corners=True):\n","    n, c, h, w = input.shape\n","    dh, dw = size\n"," \n","    input = input.view([n * c, 1, h, w])\n"," \n","    if dh < h:\n","        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n","        pad_h = (kernel_h.shape[0] - 1) // 2\n","        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n","        input = F.conv2d(input, kernel_h[None, None, :, None])\n"," \n","    if dw < w:\n","        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n","        pad_w = (kernel_w.shape[0] - 1) // 2\n","        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n","        input = F.conv2d(input, kernel_w[None, None, None, :])\n"," \n","    input = input.view([n, c, h, w])\n","    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n"," \n","class ReplaceGrad(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, x_forward, x_backward):\n","        ctx.shape = x_backward.shape\n","        return x_forward\n"," \n","    @staticmethod\n","    def backward(ctx, grad_in):\n","        return None, grad_in.sum_to_size(ctx.shape)\n"," \n","replace_grad = ReplaceGrad.apply\n"," \n","class ClampWithGrad(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, input, min, max):\n","        ctx.min = min\n","        ctx.max = max\n","        ctx.save_for_backward(input)\n","        return input.clamp(min, max)\n"," \n","    @staticmethod\n","    def backward(ctx, grad_in):\n","        input, = ctx.saved_tensors\n","        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n"," \n"," \n","clamp_with_grad = ClampWithGrad.apply\n"," \n","def vector_quantize(x, codebook):\n","    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n","    indices = d.argmin(-1)\n","    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n","    return replace_grad(x_q, x)\n"," \n","class Prompt(nn.Module):\n","    ''' Prompt model\n","      Auxiliary model to perform inferece-by-optimization\n","      computes the loss between image cutouts and text prompt\n","    '''\n","    def __init__(self, embed, weight=1., stop=float('-inf')):\n","        super().__init__()\n","        self.register_buffer('embed', embed)\n","        self.register_buffer('weight', torch.as_tensor(weight))\n","        self.register_buffer('stop', torch.as_tensor(stop))\n"," \n","    def forward(self, input):\n","        ''' Compute Similarity '''\n","        # CLIP encoded image cutout batch\n","        input_normed = F.normalize(input.unsqueeze(1), dim=2) \n","        # CLIP encoded text prompt\n","        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n","        # Distance\n","        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n","        dists = dists * self.weight.sign()\n","        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n"," \n","def parse_prompt(prompt):\n","    ''' Parse text prompt of the form \"prompt: weight: stop\"\n","    '''\n","    vals = prompt.rsplit(':', 2)\n","    vals = vals + ['', '1', '-inf'][len(vals):]\n","    return vals[0], float(vals[1]), float(vals[2])\n"," \n","class MakeCutouts(nn.Module):\n","    ''' Image cutout and augmentation module '''\n","    def __init__(self, cut_size, cutn, cut_pow=1.):\n","        super().__init__()\n","        self.cut_size = cut_size\n","        self.cutn = cutn\n","        self.cut_pow = cut_pow\n","        self.noise_fac = 0.1\n","        self.augs = nn.Sequential(\n","            K.RandomHorizontalFlip(p=0.5),\n","            K.RandomSharpness(0.3, p=0.4),\n","            K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n","            K.RandomPerspective(0.2, p=0.4),\n","            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7)\n","        )\n","\n","    def forward(self, input):\n","        sideY, sideX = input.shape[2:4]\n","        max_size = min(sideX, sideY)\n","        min_size = min(sideX, sideY, self.cut_size)\n","        cutouts = []\n","        for _ in range(self.cutn):\n","            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n","            offsetx = torch.randint(0, sideX - size + 1, ())\n","            offsety = torch.randint(0, sideY - size + 1, ())\n","            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n","            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n","        batch = self.augs(torch.cat(cutouts, dim=0))\n","        if self.noise_fac:\n","            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n","            batch = batch + facs * torch.randn_like(batch)\n","        return batch\n","\n","def load_vqgan_model(config_path, checkpoint_path):\n","    ''' Init VQGAN model in evaluation mode '''\n","    config = OmegaConf.load(config_path)\n","    if config.model.target == 'taming.models.vqgan.VQModel':\n","        model = vqgan.VQModel(**config.model.params)\n","        model.eval().requires_grad_(False)\n","        model.init_from_ckpt(checkpoint_path)\n","    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n","        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n","        parent_model.eval().requires_grad_(False)\n","        parent_model.init_from_ckpt(checkpoint_path)\n","        model = parent_model.first_stage_model\n","    else:\n","        raise ValueError(f'Unknown model type: {config.model.target}')\n","    del model.loss\n","    return model\n","\n","def resize_image(image, out_size):\n","    ratio = image.size[0] / image.size[1]\n","    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n","    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n","    return image.resize(size, Image.LANCZOS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Image as Image_\n","\n","%cd /content/icon-image/\n","!python icon_image.py --icon_name \"fab fa-spotify\" --bg_color \"white\" --icon_color \"#7b7568\" --bg_width 500 --bg_height 500 --icon_size 400\n","%cd /content\n","\n","Image_('icon-image/icon.png')"],"metadata":{"id":"v8H0R83bVEy9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Image Synthesis"],"metadata":{"id":"P9k5dvn-BznT"}},{"cell_type":"code","source":["multi_prompts = [\n","    ['colorful company logo of modern tech firm'],\n","    ['colorful company logo in the style of 1970s sci-fi book cover'],\n","    ['colorful company logo trending on artstation'],\n","    ['black and white company logo of nature lover'],\n","    ['colorful company logo of nature lover'],\n","    ['sketch of company logo of skyscraper'],\n","]"],"metadata":{"id":"XxSOgQRuEnw-"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZdlpRFL8UAlW"},"source":["for prompts in multi_prompts:\n","\n","    # retain qaudratic proportions, otherwise the output will be repetitive (CLIP only judges square sections)\n","    width, height =  512, 512\n","  \n","    # print frequency of GAN output (every `interval` iterations)\n","    interval = 50\n","\n","    # initial image to guide generation (either path as string or None)\n","    initial_image = None  # str(Path(full_path, 'wooden-banana-temple-in-an-underwater-kingdom-steampunk', '0500.png'))\n","\n","    # text prompts\n","    prompts = [p.strip() for p in prompts]\n","    \n","    # image prompts (either list of paths as string or None)\n","    image_prompts = ['/content/icon-image/icon.png'] #[str(Path(full_path, 'vulcano-temple-prompt.jpg'))]\n","    if not image_prompts:\n","        image_prompts = []\n","    else:\n","        image_prompts = [img.strip() for img in image_prompts]\n","\n","    if initial_image or image_prompts != []:\n","        input_images = True\n","\n","    # put None to get random seed\n","    seed = 2022\n","\n","    # number of total iterations (-1: infinite)\n","    max_iterations = 500  \n","\n","    args = argparse.Namespace(\n","        prompts=prompts,\n","        image_prompts=image_prompts,\n","        noise_prompt_seeds=[],\n","        noise_prompt_weights=[],\n","        size=[width, height],\n","        init_image=initial_image,\n","        init_weight=0.,\n","        clip_model='ViT-B/32',\n","        vqgan_config=f'{model_alias}.yaml',\n","        vqgan_checkpoint=f'{model_alias}.ckpt',\n","        step_size=0.1,\n","        cutn=64,\n","        cut_pow=1.,\n","        print_freq=interval,\n","        seed=seed,\n","    )\n","\n","    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","    if args.seed is None:\n","        seed = torch.seed()\n","    else:\n","        seed = args.seed\n","    torch.manual_seed(seed)\n","\n","    if prompts:\n","        print(f'Using text prompt: {prompts}')\n","    if image_prompts:\n","        print(f'Using image prompts: {image_prompts}')\n","    print(f'Using seed: {seed}')\n","\n","    # load vqgan model\n","    model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n","\n","    # download CLIP model (and freeze weights)\n","    perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n","\n","    # CLIP image encoder resolution: 224 x 224\n","    cut_size = perceptor.visual.input_resolution\n","    # codebook vector dimensionality: 256\n","    e_dim = model.quantize.e_dim\n","    # codebook size: 16384\n","    n_toks = model.quantize.n_e\n","    # ensure correct image sizing for init image and image prompt\n","    f = 2**(model.decoder.num_resolutions - 1)\n","    toksX, toksY = args.size[0] // f, args.size[1] // f\n","    sideX, sideY = toksX * f, toksY * f\n","    # min signal in codebook vectors\n","    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n","    # max signal in codebook vectors\n","    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n","    # generate `cutn` times number of augmentations cutouts\n","    make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n","\n","    # warm start from user-provided image (embed in VQ-GAN feature space)\n","    if args.init_image:\n","        pil_image = Image \\\n","            .open(args.init_image).convert('RGB') \\\n","            .resize((sideX, sideY), Image.LANCZOS)\n","        z, *_ = model.encode(TF.to_tensor(pil_image).to(device).unsqueeze(0) * 2 - 1)\n","    # alternatively init random noise vector\n","    else:\n","        one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n","        z = one_hot @ model.quantize.embedding.weight\n","        z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n","    z_orig = z.clone()\n","\n","    # optimize noise vector (*inference-by-optimization*)\n","    z.requires_grad_(True)\n","    opt = optim.Adam([z], lr=args.step_size)\n","\n","    normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n","                                     std=[0.26862954, 0.26130258, 0.27577711])\n","\n","    pMs = []\n","\n","    # parse and encode prompts in CLIP embedding space and init auxiliary Prompt model\n","    for prompt in args.prompts:\n","        txt, weight, stop = parse_prompt(prompt)\n","        embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n","        pMs.append(Prompt(embed, weight, stop).to(device))\n","\n","    for prompt in args.image_prompts:\n","        path, weight, stop = parse_prompt(prompt)\n","        img = resize_image(Image.open(path).convert('RGB'), (sideX, sideY))\n","        batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n","        embed = perceptor.encode_image(normalize(batch)).float()\n","        pMs.append(Prompt(embed, weight, stop).to(device))\n","\n","    for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n","        gen = torch.Generator().manual_seed(seed)\n","        embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n","        pMs.append(Prompt(embed, weight).to(device))\n","\n","    def synth(z):\n","        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n","        return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n","\n","    def add_xmp_data(fname):\n","        image = ImgTag(filename=fname)\n","        params = {\"prop_array_is_ordered\": True, \"prop_value_is_array\": True}\n","        image.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'creator', 'VQGAN+CLIP', params)\n","        if args.prompts:\n","            image.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', \" | \".join(args.prompts), params)\n","        else:\n","            image.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', 'None', params)\n","        image.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'i', str(i), params)\n","        image.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'model', model_alias, params)\n","        image.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'seed',str(seed) , params)\n","        image.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'input_images',str(input_images) , params)\n","        image.close()\n","\n","    def add_stegano_data(filename):\n","        ''' Encodes metadata into image via stenography\n","            Decode via: https://stylesuxx.github.io/steganography/\n","        '''\n","        data = {\"title\": \" | \".join(args.prompts) if args.prompts else None,\n","                \"notebook\": \"VQGAN+CLIP\",\n","                \"i\": i,\n","                \"model\": model_alias,\n","                \"seed\": str(seed),\n","                \"input_images\": input_images}\n","        lsb.hide(filename, json.dumps(data)).save(filename)\n","\n","    @torch.no_grad()\n","    def checkin(i, losses):\n","        losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n","        tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n","        out = synth(z)\n","        TF.to_pil_image(out[0].cpu()).save('progress.png')\n","        add_stegano_data('progress.png')\n","        add_xmp_data('progress.png')\n","        display.display(display.Image('progress.png'))\n","\n","    def ascend_txt():\n","        ''' Gradient Ascent '''\n","        # get counter from global environment \n","        global i\n","        # synthesized (i.e., decode) image based on noise vector\n","        out = synth(z)\n","        # batch of image cutouts encoded into CLIP embedding space\n","        iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n","\n","        result = []\n","\n","        if args.init_weight:\n","            result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n","\n","        # compute loss by comparing each image cutout with input text prompt(s) (loss per prompt)\n","        for prompt in pMs:\n","            result.append(prompt(iii))\n","\n","        # save image and encode metadata\n","        img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n","        img = np.transpose(img, (1, 2, 0))\n","        fname = f'steps/{i:04}.png'\n","        imageio.imwrite(fname, np.array(img))\n","        add_stegano_data(fname)\n","        add_xmp_data(fname)\n","\n","        return result\n","\n","    def train(i):\n","        opt.zero_grad()\n","        lossAll = ascend_txt()\n","        if i % args.print_freq == 0:\n","            checkin(i, lossAll)\n","        loss = sum(lossAll)\n","        loss.backward()\n","        # update noise vector Z\n","        opt.step()\n","        with torch.no_grad():\n","            z.copy_(z.maximum(z_min).minimum(z_max))\n","\n","    # actual training loop\n","    i = 0\n","    try:\n","        with tqdm() as pbar:\n","            while True:\n","                train(i)\n","                if i == max_iterations:\n","                    break\n","                i += 1\n","                pbar.update()\n","    except KeyboardInterrupt:\n","        pass\n","\n","    # save images\n","    if image_prompts:\n","      img_dir = Path(full_path, '|'.join(prompts).replace(' ', '-') + '_targeted')\n","    else:\n","      img_dir = Path(full_path, '|'.join(prompts).replace(' ', '-'))\n","\n","    if not img_dir.exists():\n","        img_dir.mkdir()\n","\n","    for i in tqdm(range(0, max_iterations + 1, args.print_freq)):\n","        fname = f'{i:04}.png'\n","        shutil.copy(\n","            Path('steps', fname),\n","            Path(img_dir, fname)\n","        )\n","\n","    ### create video\n","    init_frame = 1\n","    last_frame = max_iterations\n","    total_frames = last_frame - init_frame\n","\n","    min_fps = 10\n","    max_fps = 30\n","\n","    length = 15 # Desired video runtime in seconds\n","\n","    frames = []\n","\n","    for i in range(init_frame, last_frame):\n","        fname = f'steps/{i:04}.png'\n","        frames.append(Image.open(fname))\n","\n","    # adjust based on desired video length and min and max fps, respectively\n","    fps = np.clip(total_frames/length, min_fps, max_fps)\n","\n","    video_fname = \"video.mp4\"\n","\n","    from subprocess import Popen, PIPE\n","    cmd = ['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps),\n","           '-i', '-', '-vcodec', 'libx264', '-r', str(fps),\n","           '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', video_fname]\n","    p = Popen(cmd, stdin=PIPE)\n","\n","    for img in tqdm(frames):\n","        img.save(p.stdin, 'PNG')\n","    p.stdin.close()\n","    p.wait()\n","    print(\"Video ready\")\n","\n","    shutil.copy(\n","        Path(video_fname),\n","        Path(img_dir, video_fname)\n","    )\n","\n","\n","    ### increase resolution via SRCNN\n","    cmd = ['python', '/content/SRCNN/run.py',\n","          '--zoom_factor', f'{SR_FACTOR}',\n","          '--model', f'/content/model_{SR_FACTOR}x.pth',\n","          '--image', f'{max_iterations:04}.png',\n","          '--cuda']\n","    process = Popen(cmd, cwd=f'/content/steps')\n","    stdout, stderr = process.communicate()\n","    if process.returncode != 0:\n","        print(stderr)\n","        raise RuntimeError(stderr)\n","\n","    shutil.copy(\n","        Path('content', 'steps', f'zoomed_{max_iterations:04}.png'),\n","        Path(img_dir, f'zoomed_{max_iterations:04}.png')\n","    )"],"execution_count":null,"outputs":[]}]}